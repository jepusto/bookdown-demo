[
["index.html", "Designing Monte Carlo Simulations in R Chapter 1 Introduction 1.1 Purpose of simulations 1.2 Why R? 1.3 Setup", " Designing Monte Carlo Simulations in R James E. Pustejovsky 2017-09-19 Chapter 1 Introduction 1.1 Purpose of simulations Computer simulations are an essential tool of inquiry for statisticians and quantitative methodologists, useful both for small-scale or informal investigations and for formal methodological research. Simulation studies involve investigating the performance of a statistical model or method under known data-generating processes. By controlling the data generating process (e.g., by specifying true values for the parameters of a statistical model) and then repeatedly applying a statistical method to data generated by that process, it becomes possible to assess the accuracy of a statistical procedure, as well as other aspects of a procedure’s performance. Simulations are used to address a range of questions: Many statistical estimation procedures can be shown (through mathematical analysis) to work well asymptotically—that is, given an infinite amount of data—but their performance in small samples is more difficult to quantify. Simulation is a tractable approach for assessing the small-sample performance of such methods, or for determining minimum required sample sizes for adequate performance of a method. For example, heteroskedasticity-robust standard errors (HRSE) are known to work asymptotically, but can be misleading in small samples. Long and Ervin (2000) investigate the performance of HRSE for a range of sample sizes, demonstrating that the most commonly used form of these estimators often does NOT work well with sample sizes typical in the social sciences. In practice, statistical methods are often used in combination. For instance, in a regression model, one could first use a statistical test for heteroskedasticity (e.g., the White test or the Breusch-Pagan test) and then determine whether to use conventional standard errors or HRSE depending on the result of the test. This combination of an initial diagnostic test followed by contingent use of different statistical procedures is all but impossible to analyze mathematically, but it is straight-forward to simulate (again, see Long &amp; Ervin, 2000). Many statistical estimation procedures can be shown (through mathematical analysis) to perform well when the assumptions they entail are correct. However, it practice it is often of interest to also understand their robustness—that is, their performance when one or more of the assumptions is incorrect. There may be several potential methods available for conducting a certain analysis. Simulation is a useful way to determine whether some methods are superior to others. For instance, Brown and Forsythe (1974) compared four different procedures for conducting a hypothesis test for equality of means in several populations (i.e., one-way ANOVA), but where the population variances are not equal. The purpose of this monograph is to provide a guide to designing simulation studies to answer questions about statistical methodology. The focus is on studies used for formal research purposes (i.e., as might appear in a journal article or dissertation), although many of the programming techniques are relevant to less formal situations as well. 1.2 Why R? 1.2.1 Functions 1.2.2 Function skeletons 1.3 Setup "],
["structure-of-a-simulation-study.html", "Chapter 2 Structure of a simulation study 2.1 Structure of a simulation study 2.2 Modular simulations 2.3 A first example: Heteroskedasticity ANOVA 2.4 Exercises", " Chapter 2 Structure of a simulation study 2.1 Structure of a simulation study We can think of simulation studies as involving five distinct pieces, as illustrated in the diagram below. 2.1.1 Experimental design Simulation studies closely resemble designed experiments, in which factors such as sample size and true parameter values are systematically varied. Typically, simulation studies follow a full factorial design, in which each level of a factor is crossed with every other level. (We will look further at this aspect of simulation design in a later class.) The experimental design consists of sets of parameter values (including design parameters, such as sample sizes) that will be considered in the study. 2.1.2 Data-generating model The data-generating model takes a set of parameter values as input and generates a set of simulated data as output. 2.1.3 Estimation methods The estimation methods consist of the set of statistical procedures under examination. Each method takes a simulated dataset and produces a set of estimates or results (i.e., point estimates, standard errors, confidence intervals, p-values, etc.). 2.1.4 Performance summaries Performance summaries are the metrics used to assess the performance of a statistical method. Interest usually centers on understanding the performance of a method over repeated samples from a data-generating process. Thus, we will need to repeat steps 2 and 3 many times to get a large number of simulated estimates. We then summarize the distribution of the estimates to characterize performance of a method. 2.1.5 Results Simulation results consist of sets of performance metrics for every combination of parameter values in the experimental design. Thus, steps 2 through 4 will need to get repeated a bunch. 2.2 Modular simulations In writing R code to implement all these calculations, we will follow a modular approach, in which each component of the simulation is implemented as a separate function (or potentially a set of several functions). Writing separate functions for the different components of the simulation will make the code easier to read, test, and debug. Furthermore, it makes it possible to swap components of the simulation in or out, such as by adding additional estimation methods or trying out a data-generating model that involves different distributional assumptions. 2.3 A first example: Heteroskedasticity ANOVA To illustrate the process of programming a simulation, let’s look at the simulations from Brown and Forsythe (1974). These authors study methods for testing hypotheses in the following model. Consider a population consisting of \\(g\\) separate groups, with population means \\(\\mu_1,...,\\mu_g\\) and population variances \\(\\sigma_1^2,...,\\sigma_g^2\\) for some characteristic \\(X\\). We obtain samples of size \\(n_1,...,n_g\\) from each of the groups, and take measurements of the characteristic for each unit in each group. Let \\(x_{ij}\\) denote the measurement from unit \\(j\\) in group \\(i\\), for \\(i = 1,...,g\\) and \\(j = 1,...,n_i\\). Our goal is to use the sample data to test the hypothesis that the population means are all equal, i.e., \\[ H_0: \\mu_1 = \\mu_2 = \\cdots = \\mu_g. \\] Note that if the population variances were all equal (i.e., \\(\\sigma_1^2 = \\sigma_2^2 = \\cdots = \\sigma_g^2\\)), we could use a conventional one-way analysis of variance to test the hypothesis. However, one-way ANOVA might not work well if the variances are not equal. Brown and Forsythe evaluated two different hypothesis testing procedures, developed by James (1951) and Welch (1951), that had been proposed for testing this hypothesis without assuming equality of variances, as well as the conventional one-way ANOVA F-test as a benchmark. They also proposed and evaluated a new procedure of their own devising. The simulation involves comparing the performance of these different hypothesis testing procedures under a range of conditions. The main performance metrics of interest were the type-I error rate (i.e., when the null hypothesis is true, how often does each test falsely reject the null?) and power (how often does the test correctly reject the null when it is indeed false?) of each test, for nominal \\(\\alpha\\)-levels of 1%, 5%, and 10%. Table 1 of the paper reports the simulation results for type-I error (labeled as “size”); ideally, a test should have true type-I error very close to the nominal \\(\\alpha\\). Table 2 reports results on power; it is desirable to have higher power to reject null hypotheses that are false, so higher rates are better here. 2.3.1 Data-generating model In the heteroskedastic one-way ANOVA simulation, there are three sets of parameter values: population means, population variances, and sample sizes. Rather than attempting to write a data-generating function immediately, it is often easier to write code for a specific case first. For example, say that we have four groups with means of 1, 2, 5, 6; variances of 3, 2, 5, 1; and sample sizes of 3, 6, 2, and 4: mu &lt;- c(1, 2, 5, 6) sigma_sq &lt;- c(3, 2, 5, 1) sample_size &lt;- c(3, 6, 2, 4) Following Brown and Forsythe, we’ll assume that the measurements are normally distributed within each sub-group of the population. The following code generates a vector of group id’s and a vector of simulated measurements: N &lt;- sum(sample_size) # total sample size g &lt;- length(sample_size) # number of groups group &lt;- rep(1:g, times = sample_size) # group id mu_long &lt;- rep(mu, times = sample_size) # mean for each unit of the sample sigma_long &lt;- rep(sqrt(sigma_sq), times = sample_size) # sd for each unit of the sample cbind(group, mu_long, sigma_long) ## group mu_long sigma_long ## [1,] 1 1 1.732051 ## [2,] 1 1 1.732051 ## [3,] 1 1 1.732051 ## [4,] 2 2 1.414214 ## [5,] 2 2 1.414214 ## [6,] 2 2 1.414214 ## [7,] 2 2 1.414214 ## [8,] 2 2 1.414214 ## [9,] 2 2 1.414214 ## [10,] 3 5 2.236068 ## [11,] 3 5 2.236068 ## [12,] 4 6 1.000000 ## [13,] 4 6 1.000000 ## [14,] 4 6 1.000000 ## [15,] 4 6 1.000000 x &lt;- rnorm(N, mean = mu_long, sd = sigma_long) data.frame(group = group, x = x) ## group x ## 1 1 0.4822516 ## 2 1 1.0752085 ## 3 1 -2.4938225 ## 4 2 1.0918780 ## 5 2 1.4485053 ## 6 2 4.1764795 ## 7 2 1.5667341 ## 8 2 0.3246407 ## 9 2 2.8558169 ## 10 3 3.6728065 ## 11 3 5.5564407 ## 12 4 7.8966876 ## 13 4 6.5056311 ## 14 4 5.7699004 ## 15 4 4.7338175 Now wrap this code in a function: generate_data &lt;- function(mu, sigma_sq, sample_size) { N &lt;- sum(sample_size) g &lt;- length(sample_size) group &lt;- rep(1:g, times = sample_size) mu_long &lt;- rep(mu, times = sample_size) sigma_long &lt;- rep(sqrt(sigma_sq), times = sample_size) x &lt;- rnorm(N, mean = mu_long, sd = sigma_long) sim_data &lt;- data.frame(group = group, x = x) return(sim_data) } generate_data(mu = mu, sigma_sq = sigma_sq, sample_size = sample_size) ## group x ## 1 1 -2.0773686 ## 2 1 0.3132873 ## 3 1 2.1737720 ## 4 2 1.0551871 ## 5 2 3.4380941 ## 6 2 3.0390103 ## 7 2 2.4394515 ## 8 2 4.4604866 ## 9 2 3.3478474 ## 10 3 1.8169673 ## 11 3 4.3918378 ## 12 4 6.2176345 ## 13 4 7.9857844 ## 14 4 5.0814689 ## 15 4 6.1412172 Re-run the function and we get a new set of simulated data: generate_data(mu = mu, sigma_sq = sigma_sq, sample_size = sample_size) ## group x ## 1 1 -1.4882201 ## 2 1 2.8344801 ## 3 1 0.9330307 ## 4 2 1.8596232 ## 5 2 3.4502226 ## 6 2 1.2956599 ## 7 2 0.6343196 ## 8 2 2.0022673 ## 9 2 2.9621660 ## 10 3 10.4442654 ## 11 3 3.8240359 ## 12 4 6.7011487 ## 13 4 5.3237051 ## 14 4 5.4305371 ## 15 4 6.2220558 2.3.2 Estimation procedures Brown and Forsythe considered four different hypothesis testing procedures for heteroskedastic ANOVA. For starters, let’s look at the simplest one, which is just to use a conventional one-way ANOVA (while mistakenly assuming homoskedasticity). The oneway.test function will calculate this test automatically: sim_data &lt;- generate_data(mu = mu, sigma_sq = sigma_sq, sample_size = sample_size) oneway.test(x ~ factor(group), data = sim_data, var.equal = TRUE) ## ## One-way analysis of means ## ## data: x and factor(group) ## F = 10.609, num df = 3, denom df = 11, p-value = 0.001414 The main result we need here is the p-value, which will let us assess the test’s Type-I error and power for a given nominal \\(\\alpha\\)-level. The following function takes simulated data as input and returns as output the p-value from a one-way ANOVA: ANOVA_F_aov &lt;- function(sim_data) { oneway_anova &lt;- oneway.test(x ~ factor(group), data = sim_data, var.equal = TRUE) return(oneway_anova$p.value) } ANOVA_F_aov(sim_data) ## [1] 0.001414148 An alternative approach would be to program the ANOVA F statistic and test directly. Following the formulas on p. 129 of Brown and Forsythe (1974): ANOVA_F_direct &lt;- function(sim_data) { x_bar &lt;- with(sim_data, tapply(x, group, mean)) s_sq &lt;- with(sim_data, tapply(x, group, var)) n &lt;- table(sim_data$group) g &lt;- length(x_bar) df1 &lt;- g - 1 df2 &lt;- sum(n) - g msbtw &lt;- sum(n * (x_bar - mean(sim_data$x))^2) / df1 mswn &lt;- sum((n - 1) * s_sq) / df2 fstat &lt;- msbtw / mswn pval &lt;- pf(fstat, df1, df2, lower.tail = FALSE) return(pval) } ANOVA_F_direct(sim_data) ## [1] 0.001414148 This approach takes more work to program, but will end up being quicker to compute. Now let’s consider another one of the tests considered by Brown and Forsythe. Here is a function that calculates the Welch test, again following the notation and formulas from the paper: Welch_F &lt;- function(sim_data) { x_bar &lt;- with(sim_data, tapply(x, group, mean)) s_sq &lt;- with(sim_data, tapply(x, group, var)) n &lt;- table(sim_data$group) g &lt;- length(x_bar) w &lt;- n / s_sq u &lt;- sum(w) x_tilde &lt;- sum(w * x_bar) / u msbtw &lt;- sum(w * (x_bar - x_tilde)^2) / (g - 1) G &lt;- sum((1 - w / u)^2 / (n - 1)) denom &lt;- 1 + G * 2 * (g - 2) / (g^2 - 1) W &lt;- msbtw / denom f &lt;- (g^2 - 1) / (3 * G) pval &lt;- pf(W, df1 = g - 1, df2 = f, lower.tail = FALSE) return(pval) } Welch_F(sim_data) ## [1] 0.01112745 2.3.3 Replicate We now have functions that implement steps 2 and 3 of the simulation. Starting with the parameters, generate_data produces a simulated dataset and ANOVA_F and Welch_F use the simulated data to calculate p-values. To evaluate the accuracy of these tests, we now need to repeat this chain of calculations a bunch of times. R includes a function called replicate that will be very handy as we design and implement simulations. The function does what its name suggests—it replicates the result of an expression a specified number of times. The first argument is the number of times to replicate and the next argument is an expression (a short piece of code to run). A further argument, simplify allows you to control how the results are structured. Setting simplify = FALSE returns the output as a list. The following code produces 4 replications of the simulated dataset: sim_data &lt;- replicate(n = 4, generate_data(mu = mu, sigma_sq = sigma_sq, sample_size = sample_size), simplify = FALSE) str(sim_data) ## List of 4 ## $ :&#39;data.frame&#39;: 15 obs. of 2 variables: ## ..$ group: int [1:15] 1 1 1 2 2 2 2 2 2 3 ... ## ..$ x : num [1:15] 1.2647 -0.2778 -0.0568 2.1726 4.0633 ... ## $ :&#39;data.frame&#39;: 15 obs. of 2 variables: ## ..$ group: int [1:15] 1 1 1 2 2 2 2 2 2 3 ... ## ..$ x : num [1:15] 2.4746 2.0527 1.7435 0.0836 2.0024 ... ## $ :&#39;data.frame&#39;: 15 obs. of 2 variables: ## ..$ group: int [1:15] 1 1 1 2 2 2 2 2 2 3 ... ## ..$ x : num [1:15] 0.594 1.441 1.161 0.849 0.353 ... ## $ :&#39;data.frame&#39;: 15 obs. of 2 variables: ## ..$ group: int [1:15] 1 1 1 2 2 2 2 2 2 3 ... ## ..$ x : num [1:15] 1.746 -0.141 0.824 1.248 2.275 ... You can put multiple lines of code in the expression by enclosing them in curly brackets ({}). The following code produces 4 replications of the test statistics and p-value from Welch’s test: replicate(n = 4, { sim_data &lt;- generate_data(mu = mu, sigma_sq = sigma_sq, sample_size = sample_size) Welch_F(sim_data) }, simplify = FALSE) ## [[1]] ## [1] 0.02880681 ## ## [[2]] ## [1] 0.02315537 ## ## [[3]] ## [1] 0.001157549 ## ## [[4]] ## [1] 0.009491997 However, we need to get results from the ANOVA F-test as well. Let’s add that in and just return a data frame with p-values: replicate(n = 4, { sim_data &lt;- generate_data(mu = mu, sigma_sq = sigma_sq, sample_size = sample_size) anova_p &lt;- ANOVA_F_direct(sim_data) Welch_p &lt;- Welch_F(sim_data) data.frame(ANOVA = anova_p, Welch = Welch_p) }, simplify = FALSE) ## [[1]] ## ANOVA Welch ## 1 0.0007396765 0.06939443 ## ## [[2]] ## ANOVA Welch ## 1 0.002265724 0.01577381 ## ## [[3]] ## ANOVA Welch ## 1 0.001612422 0.0571153 ## ## [[4]] ## ANOVA Welch ## 1 0.02272783 0.07474894 The bind_rows function from the dplyr package will turn this list into a data frame, for easier manipulation: library(dplyr) p_vals &lt;- replicate(n = 4, { sim_data &lt;- generate_data(mu = mu, sigma_sq = sigma_sq, sample_size = sample_size) anova_p &lt;- ANOVA_F_direct(sim_data) Welch_p &lt;- Welch_F(sim_data) data.frame(ANOVA = anova_p, Welch = Welch_p) }, simplify = FALSE) bind_rows(p_vals) ## ANOVA Welch ## 1 1.332414e-04 0.002647816 ## 2 5.383084e-05 0.012119009 ## 3 5.078403e-02 0.019153694 ## 4 1.815768e-03 0.045761493 Voila! Simulated p-values! 2.3.4 Calculating rejection rates We’ve got all the pieces in place now to reproduce the results from Brown and Forsythe (1974). Let’s focus on calculating the actual type-I error rate of these tests—that is, the proportion of the time that they reject the null hypothesis of equal means when that null is actually true—for an \\(\\alpha\\)-level of .05. We therefore need to simulate data according to process where the population means are indeed all equal. Arbitrarily, let’s look at \\(g = 4\\) groups and set all of the means equal to zero: mu &lt;- rep(0, 4) In the fifth row of Table 1, they examine performance for the following parameter values for sample size and population variance: sample_size &lt;- c(4, 8, 10, 12) sigma_sq &lt;- c(3, 2, 2, 1)^2 With these parameter values, we can use our replicate code to simulate 10,000 p-values: p_vals &lt;- replicate(n = 10000, { sim_data &lt;- generate_data(mu = mu, sigma_sq = sigma_sq, sample_size = sample_size) anova_p &lt;- ANOVA_F_direct(sim_data) Welch_p &lt;- Welch_F(sim_data) data.frame(ANOVA = anova_p, Welch = Welch_p) }, simplify = FALSE) p_vals &lt;- bind_rows(p_vals) head(p_vals) ## ANOVA Welch ## 1 0.9224926 0.9348893 ## 2 0.2931098 0.4921055 ## 3 0.4004571 0.6817363 ## 4 0.5388679 0.4597107 ## 5 0.4740096 0.5876007 ## 6 0.2582931 0.5352149 Now how to calculate the rejection rates? The rule is that the null is rejected if the p-value is less than \\(\\alpha\\). To get the rejection rate, calculate the proportion of replications where the null is rejected. This is equivalent to taking the mean of the logical conditions: mean(p_vals$ANOVA &lt; .05) ## [1] 0.1414 We get a rejection rate that is much larger than \\(\\alpha = .05\\), which indicates that the ANOVA F-test does not adequately control Type-I error under this set of conditions. mean(p_vals$Welch &lt; .05) ## [1] 0.0644 The Welch test does much better, although it is still a little bit in excess of .05. Note that these two numbers are quite close (though not quite identical) to the corresponding entries in Table 1 of Brown and Forsythe (1974). The difference is due to the fact that both Table 1 and are results are actually estimated rejection rates, because we haven’t actually simulated an infinite number of replications. The estimation error arising from using a finite number of replications is called simulation error (or Monte Carlo error). In a later class, we’ll look more at how to estimate and control the simulation error in our studies. 2.4 Exercises The following exercises involve exploring and tweaking the simulation code we’ve developed to replicate the results of Brown and Forsythe (1974). Here are the key functions for the data-generating process and estimation procedures: generate_data &lt;- function(mu, sigma_sq, sample_size) { N &lt;- sum(sample_size) g &lt;- length(sample_size) group &lt;- rep(1:g, times = sample_size) mu_long &lt;- rep(mu, times = sample_size) sigma_long &lt;- rep(sqrt(sigma_sq), times = sample_size) x &lt;- rnorm(N, mean = mu_long, sd = sigma_long) sim_data &lt;- data.frame(group = group, x = x) return(sim_data) } summarize_data &lt;- function(sim_data) { x_bar &lt;- with(sim_data, tapply(x, group, mean)) s_sq &lt;- with(sim_data, tapply(x, group, var)) n &lt;- table(sim_data$group) g &lt;- length(x_bar) return(list(x_bar = x_bar, s_sq = s_sq, n = n, g = g)) } ANOVA_F &lt;- function(x_bar, s_sq, n, g) { df1 &lt;- g - 1 df2 &lt;- sum(n) - g msbtw &lt;- sum(n * (x_bar - weighted.mean(x_bar, w = n))^2) / df1 mswn &lt;- sum((n - 1) * s_sq) / df2 fstat &lt;- msbtw / mswn pval &lt;- pf(fstat, df1, df2, lower.tail = FALSE) return(pval) } Welch_F &lt;- function(x_bar, s_sq, n, g) { w &lt;- n / s_sq u &lt;- sum(w) x_tilde &lt;- sum(w * x_bar) / u msbtw &lt;- sum(w * (x_bar - x_tilde)^2) / (g - 1) G &lt;- sum((1 - w / u)^2 / (n - 1)) denom &lt;- 1 + G * 2 * (g - 2) / (g^2 - 1) W &lt;- msbtw / denom f &lt;- (g^2 - 1) / (3 * G) pval &lt;- pf(W, df1 = g - 1, df2 = f, lower.tail = FALSE) return(data.frame(W = W, f = f, pval = pval)) } Table 1 from Brown and Forsythe reported rejection rates for \\(\\alpha = .01\\) and \\(\\alpha = .10\\) in addition to \\(\\alpha = .05\\). Calculate the rejection rates of the ANOVA F and Welch tests for all three \\(\\alpha\\)-levels. Use the dataset of p-values calculated in the Class 12 notes, which is loaded below. head(p_vals) ## ANOVA Welch ## 1 0.9224926 0.9348893 ## 2 0.2931098 0.4921055 ## 3 0.4004571 0.6817363 ## 4 0.5388679 0.4597107 ## 5 0.4740096 0.5876007 ## 6 0.2582931 0.5352149 Try simulating the Type-I error rates for a the parameter values in the first two rows of Table 1. Use 10,000 replications. How do your results compare to the results reported in the Table? Try simulating the power levels for a couple of sets of parameter values from Table 2. Use 10,000 replications. How do your results compare to the results reported in the Table? "],
["data-generating-models.html", "Chapter 3 Data-generating models 3.1 Efficiency versus simplicity 3.2 Checking the data-generating function 3.3 Exercises", " Chapter 3 Data-generating models In the abstract, a function that implements a data-generating model should have the following form: generate_data &lt;- function(parameters) { # simulations and calculations return(sim_data) } The function takes a set of parameter values as input, simulates random numbers and does calculations, and produces as output a set of simulated data. In practice, the parameters will typically consist of multiple values, including not only the model parameters, but also sample sizes and other study design parameters. generate_data &lt;- function(mu, sigma_sq, sample_size) { N &lt;- sum(sample_size) g &lt;- length(sample_size) group &lt;- rep(1:g, times = sample_size) mu_long &lt;- rep(mu, times = sample_size) sigma_long &lt;- rep(sqrt(sigma_sq), times = sample_size) x &lt;- rnorm(N, mean = mu_long, sd = sigma_long) sim_data &lt;- data.frame(group = group, x = x) return(sim_data) } generate_data(mu = mu, sigma_sq = sigma_sq, sample_size = sample_size) ## group x ## 1 1 -6.10311699 ## 2 1 7.09779010 ## 3 1 -3.05324570 ## 4 1 0.17916851 ## 5 2 -5.79803792 ## 6 2 2.12062853 ## 7 2 -1.32251409 ## 8 2 1.37889305 ## 9 2 -5.27709299 ## 10 2 3.01442148 ## 11 2 0.96792425 ## 12 2 1.10883293 ## 13 3 0.79363153 ## 14 3 1.13525361 ## 15 3 -1.76916191 ## 16 3 0.85537834 ## 17 3 -1.44973443 ## 18 3 0.94222524 ## 19 3 -0.68975036 ## 20 3 -2.51196256 ## 21 3 -0.54302904 ## 22 3 -2.08463159 ## 23 4 -0.76333201 ## 24 4 -0.12681325 ## 25 4 -1.27165406 ## 26 4 0.40327601 ## 27 4 0.35994012 ## 28 4 0.85735394 ## 29 4 0.85166433 ## 30 4 -1.60296844 ## 31 4 0.44923101 ## 32 4 -0.13996682 ## 33 4 0.05025531 ## 34 4 -0.57785882 3.1 Efficiency versus simplicity An alternative approach to the above would be to write a function that generates multiple sets of simulated data all at once. For example, we could specify that we want R replications of the study and have the function spit out a matrix with R columns, one for each simulated dataset: generate_data_matrix &lt;- function(mu, sigma_sq, sample_size, R) { N &lt;- sum(sample_size) g &lt;- length(sample_size) group &lt;- rep(1:g, times = sample_size) mu_long &lt;- rep(mu, times = sample_size) sigma_long &lt;- rep(sqrt(sigma_sq), times = sample_size) x_mat &lt;- matrix(rnorm(N * R, mean = mu_long, sd = sigma_long), nrow = N, ncol = R) sim_data &lt;- list(group = group, x_mat = x_mat) return(sim_data) } generate_data_matrix(mu = mu, sigma_sq = sigma_sq, sample_size = sample_size, R = 4) ## $group ## [1] 1 1 1 1 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 4 4 ## ## $x_mat ## [,1] [,2] [,3] [,4] ## [1,] 0.76486732 3.44125738 3.7339370 -2.8888381 ## [2,] -0.73778082 -8.60045903 -3.3203720 -2.8363138 ## [3,] 4.29547298 -0.36009534 2.6941573 -3.0149211 ## [4,] 2.18456857 -4.98537750 2.3286105 2.1782329 ## [5,] 0.59396322 1.11697223 0.5825800 1.4983302 ## [6,] -1.63653446 -0.92386860 -1.4988349 -0.8222567 ## [7,] 2.22558051 0.82275170 1.4439232 1.0162270 ## [8,] -0.31786670 2.02091916 4.8046263 -0.9507920 ## [9,] 3.27077230 -2.44587326 4.0858552 -0.4650251 ## [10,] 1.99566773 0.54749955 -3.1861331 -1.9059702 ## [11,] -0.87120447 -0.40192781 -1.3321494 0.4772764 ## [12,] 0.50052661 -0.32031618 -3.9899096 2.5766436 ## [13,] 0.62390184 -0.83341587 0.5336018 -1.2159981 ## [14,] -2.85589793 1.25568890 0.6012132 2.2213915 ## [15,] 0.57986763 -0.75087351 -1.5702242 -2.2023996 ## [16,] -0.94534182 0.13791556 -1.5086615 0.8010087 ## [17,] -0.03857506 -6.15691691 0.8327428 2.2647281 ## [18,] 1.82342509 0.80354336 1.6008707 -2.5740698 ## [19,] 1.06658741 1.24462571 0.3843314 1.0424846 ## [20,] 3.62689376 0.22766945 -0.7757762 3.1012363 ## [21,] 2.25029026 0.80165372 1.8867487 1.7119446 ## [22,] 1.74220361 -2.34890673 1.7209389 2.4519458 ## [23,] -0.68573676 1.02734935 -0.2043652 0.7003166 ## [24,] -0.74758766 0.08314062 -0.5800473 0.3396207 ## [25,] -0.11523030 0.80947961 -0.4403020 0.5657516 ## [26,] -1.00784095 0.12989273 0.5638295 -0.7109524 ## [27,] -1.85218055 -2.13147093 0.1634373 1.6415943 ## [28,] -0.10560032 0.57882230 -1.3196482 -0.0604276 ## [29,] 0.04962952 -0.25817752 -0.8406929 -1.1286295 ## [30,] 1.23134170 -1.12542436 -1.8356413 2.2320839 ## [31,] -1.05751286 -1.68922483 0.3061455 1.0904476 ## [32,] 0.59088037 0.95971441 0.1986480 0.5827271 ## [33,] 0.24246156 0.70259174 0.2415890 -0.4407022 ## [34,] 1.07966194 1.60126788 -0.8607430 0.6274530 This approach is a bit more computationally efficient because the setup calculations (getting N, g, group, mu_full, and sigma_full) only have to be done once instead of once per replication. It also makes clever use of vector recycling in the call to rnorm(). However, the structure of the resulting data is more complicated, which will make it more difficult to do the later estimation steps. Furthermore, if R is large and each replication produces a large dataset, this “all-at-once” approach will entail generating and holding very large amounts of data in memory, which can create other performance issues. On balance, I recommend following the simpler approach of writing a function that generates a single simulated dataset per call (unless and until you have a principled reason to do otherwise). 3.2 Checking the data-generating function An important part of learning to program in R—particularly learning to write functions—is finding ways to test and check the correctness of your code. Thus, after writing a data-generating function, we need to consider how to test whether the output it produces is correct. How best to do this will depend on the data-generating model being implemented. For the heteroskedastic ANOVA problem, one basic thing we could do is check that the simulated data from each group follows a normal distribution. By generating very large samples from each group, we can effectively check characteristics of the population distribution. In the following code, I simulate very large samples from each of the four groups, check that the means and variances agree with the input parameters, and check normality using QQ plots: check_data &lt;- generate_data(mu = mu, sigma_sq = sigma_sq, sample_size = rep(10000, 4)) table(check_data$group) # check sample sizes ## ## 1 2 3 4 ## 10000 10000 10000 10000 with(check_data, tapply(x, group, mean)) # calculate means by group ## 1 2 3 4 ## -0.042370901 -0.023727523 0.036373658 -0.002048172 mu # compare to mean parameters ## [1] 0 0 0 0 with(check_data, tapply(x, group, var)) # calculate variances by group ## 1 2 3 4 ## 8.822076 4.058049 4.027327 1.012325 sigma_sq # compare to variance parameters ## [1] 9 4 4 1 # check normality with(check_data, qqnorm(x[group==1])) with(check_data, qqnorm(x[group==2])) with(check_data, qqnorm(x[group==3])) with(check_data, qqnorm(x[group==4])) 3.3 Exercises 3.3.1 Shifted-and-scaled t distribution The shifted-and-scaled t distribution has parameters \\(\\mu\\) (mean), \\(\\sigma\\) (scale), and \\(\\nu\\) (degrees of freedom). If \\(T\\) follows a student’s t distribution with \\(\\nu\\) degrees of freedom, then \\(S = \\mu + \\sigma T\\) follows a shifted-and-scaled t distribution. The following function will generate random draws from this distribution: r_tss &lt;- function(n, mean, sd, df) { mean + sd * rt(n = n, df = df) } r_tss(n = 8, mean = 3, sd = 2, df = 5) ## [1] 1.504787 2.271967 4.622572 2.291272 2.369422 -0.379155 2.485015 ## [8] 2.448134 Modify that simulate_data function to generate data from shifted-and-scaled t distributions rather than from normal distributions. Include the degrees of freedom as an input argument. Re-run the Type-I error rate calculations from the previous question. Do the results change substantially? "],
["estimation-procedures-1.html", "Chapter 4 Estimation procedures 4.1 Efficiency considerations 4.2 Checking the estimation function(s) 4.3 Exercises", " Chapter 4 Estimation procedures In the abstract, a function that implements an estimation procedure should have the following form: estimate &lt;- function(sim_data) { # calculations/model-fitting/estimation procedures return(estimates) } The function takes a set of simulated data as input, fits a model or otherwise calculates a set of estimates, and produces as output a set estimates. The estimates could be point-estimates of parameters, standard errors, confidence intervals, etc. Depending on the research question, this function might involve use of a combination of several procedures (e.g., a diagnostic test for heteroskedasticity, followed by the conventional formula or heteroskedasticity-robust formula for standard errors). Also depending on the research question, we might need to create several functions that implement different estimation procedures to be compared. Brown and Forsythe considered four different hypothesis testing procedures for heteroskedastic ANOVA. For starters, let’s look at the simplest one, which is just to use a conventional one-way ANOVA (while mistakenly assuming homoskedasticity). The oneway.test function will calculate this test automatically: sim_data &lt;- generate_data(mu = mu, sigma_sq = sigma_sq, sample_size = sample_size) oneway.test(x ~ factor(group), data = sim_data, var.equal = TRUE) ## ## One-way analysis of means ## ## data: x and factor(group) ## F = 1.4487, num df = 3, denom df = 30, p-value = 0.2483 The main result we need here is the p-value, which will let us assess the test’s Type-I error and power for a given nominal \\(\\alpha\\)-level. The following function takes simulated data as input and returns as output the p-value from a one-way ANOVA: ANOVA_F_aov &lt;- function(sim_data) { oneway_anova &lt;- oneway.test(x ~ factor(group), data = sim_data, var.equal = TRUE) return(oneway_anova$p.value) } ANOVA_F_aov(sim_data) ## [1] 0.2483031 An alternative approach would be to program the ANOVA F statistic and test directly. Following the formulas on p. 129 of Brown and Forsythe (1974): ANOVA_F_direct &lt;- function(sim_data) { x_bar &lt;- with(sim_data, tapply(x, group, mean)) s_sq &lt;- with(sim_data, tapply(x, group, var)) n &lt;- table(sim_data$group) g &lt;- length(x_bar) df1 &lt;- g - 1 df2 &lt;- sum(n) - g msbtw &lt;- sum(n * (x_bar - mean(sim_data$x))^2) / df1 mswn &lt;- sum((n - 1) * s_sq) / df2 fstat &lt;- msbtw / mswn pval &lt;- pf(fstat, df1, df2, lower.tail = FALSE) return(pval) } ANOVA_F_direct(sim_data) ## [1] 0.2483031 This approach takes more work to program, but will end up being quicker to compute. To see the difference, we’ll use an R package called microbenchmark to test how long the computations take for each version of the function. The microbenchmark function runs each expression 100 times (by default) and tracks how long the computations take. It then summarizes the distribution of timings: library(microbenchmark) timings &lt;- microbenchmark(Rfunction = ANOVA_F_aov(sim_data), direct = ANOVA_F_direct(sim_data)) timings ## Unit: microseconds ## expr min lq mean median uq max neval ## Rfunction 1073.957 1110.559 1232.1837 1127.8575 1188.885 3915.630 100 ## direct 594.315 624.356 665.0061 638.4915 685.986 811.927 100 The direct function is 1.9 times faster than the built-in R function. This result is pretty typical. Built-in R functions usually include lots of checks and error-handling, which take time to compute. These checks are crucial for messy, real-world data analysis but unnecessary with our pristine, simulated data. Here we can skip them by doing the calculations directly. Now let’s consider another one of the tests considered by Brown and Forsythe. Here is a function that calculates the Welch test, again following the notation and formulas from the paper: Welch_F &lt;- function(sim_data) { x_bar &lt;- with(sim_data, tapply(x, group, mean)) s_sq &lt;- with(sim_data, tapply(x, group, var)) n &lt;- table(sim_data$group) g &lt;- length(x_bar) w &lt;- n / s_sq u &lt;- sum(w) x_tilde &lt;- sum(w * x_bar) / u msbtw &lt;- sum(w * (x_bar - x_tilde)^2) / (g - 1) G &lt;- sum((1 - w / u)^2 / (n - 1)) denom &lt;- 1 + G * 2 * (g - 2) / (g^2 - 1) W &lt;- msbtw / denom f &lt;- (g^2 - 1) / (3 * G) pval &lt;- pf(W, df1 = g - 1, df2 = f, lower.tail = FALSE) return(pval) } Welch_F(sim_data) ## [1] 0.4172818 4.1 Efficiency considerations Computational efficiency is usually a secondary consideration when you’re starting to design a simulation study. It’s better to produce accurate code, even if it’s a bit slow, than to use write code that is speedy but hard to follow (or even worse, that produces incorrect results). All that said, there is some glaring redundancy in the two functions we’ve looked at so far. Both of them start by taking the simulated data and calculating summary statistics for each group, using the following code: x_bar &lt;- with(sim_data, tapply(x, group, mean)) s_sq &lt;- with(sim_data, tapply(x, group, var)) n &lt;- table(sim_data$group) g &lt;- length(x_bar) In the interest of not repeating ourselves, it would better to pull this code out as a separate function and then re-write the ANOVA_F and Welch_F functions to take the summary statistics as input. Here is a function that takes simulated data and returns a list of summary statistics: summarize_data &lt;- function(sim_data) { x_bar &lt;- with(sim_data, tapply(x, group, mean)) s_sq &lt;- with(sim_data, tapply(x, group, var)) n &lt;- table(sim_data$group) g &lt;- length(x_bar) return(list(x_bar = x_bar, s_sq = s_sq, n = n, g = g)) } summarize_data(sim_data) ## $x_bar ## 1 2 3 4 ## -0.3545704 -0.9992128 1.3785627 0.2061606 ## ## $s_sq ## 1 2 3 4 ## 13.824046 11.439700 5.319099 1.305822 ## ## $n ## ## 1 2 3 4 ## 4 8 10 12 ## ## $g ## [1] 4 Now we can re-write the F-test functions to use the output of this function: ANOVA_F &lt;- function(x_bar, s_sq, n, g) { df1 &lt;- g - 1 df2 &lt;- sum(n) - g msbtw &lt;- sum(n * (x_bar - weighted.mean(x_bar, w = n))^2) / df1 mswn &lt;- sum((n - 1) * s_sq) / df2 fstat &lt;- msbtw / mswn pval &lt;- pf(fstat, df1, df2, lower.tail = FALSE) return(pval) } summary_stats &lt;- summarize_data(sim_data) with(summary_stats, ANOVA_F(x_bar = x_bar, s_sq = s_sq, n = n, g = g)) ## [1] 0.2483031 Welch_F &lt;- function(x_bar, s_sq, n, g) { w &lt;- n / s_sq u &lt;- sum(w) x_tilde &lt;- sum(w * x_bar) / u msbtw &lt;- sum(w * (x_bar - x_tilde)^2) / (g - 1) G &lt;- sum((1 - w / u)^2 / (n - 1)) denom &lt;- 1 + G * 2 * (g - 2) / (g^2 - 1) W &lt;- msbtw / denom f &lt;- (g^2 - 1) / (3 * G) pval &lt;- pf(W, df1 = g - 1, df2 = f, lower.tail = FALSE) return(data.frame(W = W, f = f, pval = pval)) } with(summary_stats, Welch_F(x_bar = x_bar, s_sq = s_sq, n = n, g = g)) ## W f pval ## 1 1.043552 9.458841 0.4172818 The results are the same as before. 4.2 Checking the estimation function(s) Just as with the data-generating function, it is important to verify the accuracy of the estimation functions. For the ANOVA-F test, this can be done simply by checking the result of ANOVA_F against the built-in oneway.test function. Let’s do that with a fresh set of data: sim_data &lt;- generate_data(mu = mu, sigma_sq = sigma_sq, sample_size = sample_size) aov_results &lt;- oneway.test(x ~ factor(group), data = sim_data, var.equal = TRUE) aov_results ## ## One-way analysis of means ## ## data: x and factor(group) ## F = 2.5599, num df = 3, denom df = 30, p-value = 0.07358 summary_stats &lt;- summarize_data(sim_data) F_results &lt;- with(summary_stats, ANOVA_F(x_bar = x_bar, s_sq = s_sq, n = n, g = g)) F_results ## [1] 0.07357847 all.equal(aov_results$p.value, F_results) ## [1] TRUE We can follow the same approach to check the results of the Welch test because it is also implemented in oneway.test: aov_results &lt;- oneway.test(x ~ factor(group), data = sim_data, var.equal = FALSE) aov_results ## ## One-way analysis of means (not assuming equal variances) ## ## data: x and factor(group) ## F = 2.5941, num df = 3.000, denom df = 10.468, p-value = 0.1081 W_results &lt;- with(summary_stats, Welch_F(x_bar = x_bar, s_sq = s_sq, n = n, g = g)) W_results ## W f pval ## 1 2.594102 10.46754 0.1080978 all.equal(aov_results$p.value, W_results$pval) ## [1] TRUE Checking estimation functions can be a bit more difficult for procedures that are not already implemented in R. For example, the two other procedures examined by Brown and Forsythe, the James’ test and Brown and Forsythe’s \\(F*\\) test, are not available in base R. They are available in the user-contributed package onewaytests (I found this by searching for “Brown-Forsythe” at http://rseek.org/). We could benchmark our calculations against this package, but of course there is some risk that the package might not be correct. Another route is to verify your results on numerical examples reported in authoritative papers, on the assumption that there’s less risk of an error there. In the original paper that proposed the test, Welch (1951) provides a worked numerical example of the procedure. He reports the following summary statistics: g &lt;- 3 x_bar &lt;- c(27.8, 24.1, 22.2) s_sq &lt;- c(60.1, 6.3, 15.4) n &lt;- c(20, 20, 10) He also reports \\(W = 3.35\\) and \\(f = 22.6\\). Replicating the calculations with our Welch_F function: Welch_F(x_bar = x_bar, s_sq = s_sq, n = n, g = g) ## W f pval ## 1 3.344116 21.06575 0.05479049 We get slightly different results! But we know that our function is correct—or at least consistent with oneway.test—so what’s going on? It turns out that there was an error in some of Welch’s intermediate calculations, which can only be spotted because he reported all of his work in the paper. 4.3 Exercises The following exercises involve exploring and tweaking the simulation code we’ve developed to replicate the results of Brown and Forsythe (1974). Here are the key functions for the data-generating process and estimation procedures: generate_data &lt;- function(mu, sigma_sq, sample_size) { N &lt;- sum(sample_size) g &lt;- length(sample_size) group &lt;- rep(1:g, times = sample_size) mu_long &lt;- rep(mu, times = sample_size) sigma_long &lt;- rep(sqrt(sigma_sq), times = sample_size) x &lt;- rnorm(N, mean = mu_long, sd = sigma_long) sim_data &lt;- data.frame(group = group, x = x) return(sim_data) } summarize_data &lt;- function(sim_data) { x_bar &lt;- with(sim_data, tapply(x, group, mean)) s_sq &lt;- with(sim_data, tapply(x, group, var)) n &lt;- table(sim_data$group) g &lt;- length(x_bar) return(list(x_bar = x_bar, s_sq = s_sq, n = n, g = g)) } ANOVA_F &lt;- function(x_bar, s_sq, n, g) { df1 &lt;- g - 1 df2 &lt;- sum(n) - g msbtw &lt;- sum(n * (x_bar - weighted.mean(x_bar, w = n))^2) / df1 mswn &lt;- sum((n - 1) * s_sq) / df2 fstat &lt;- msbtw / mswn pval &lt;- pf(fstat, df1, df2, lower.tail = FALSE) return(pval) } Welch_F &lt;- function(x_bar, s_sq, n, g) { w &lt;- n / s_sq u &lt;- sum(w) x_tilde &lt;- sum(w * x_bar) / u msbtw &lt;- sum(w * (x_bar - x_tilde)^2) / (g - 1) G &lt;- sum((1 - w / u)^2 / (n - 1)) denom &lt;- 1 + G * 2 * (g - 2) / (g^2 - 1) W &lt;- msbtw / denom f &lt;- (g^2 - 1) / (3 * G) pval &lt;- pf(W, df1 = g - 1, df2 = f, lower.tail = FALSE) return(data.frame(W = W, f = f, pval = pval)) } 4.3.1 Estimation helper function Write a helper function that rolls up the summarize_data, ANOVA_F, and Welch_F functions and outputs p-values for the ANOVA F-test and the Welch test. Here’s a skeleton to fill in: calculate_pvals &lt;- function(sim_data) { # fill in the guts here return(pvals) } Use the helper function to simplify the following replicate call (you’ll need to change eval to TRUE in the header of this code chunk for the code to compile when you knit): mu &lt;- rep(0, 4) sample_size &lt;- c(4, 8, 10, 12) sigma_sq &lt;- c(3, 2, 2, 1)^2 p_vals &lt;- replicate(n = 10000, { sim_data &lt;- generate_data(mu = mu, sigma_sq = sigma_sq, sample_size = sample_size) summary_stats &lt;- summarize_data(sim_data) anova_p &lt;- with(summary_stats, ANOVA_F(x_bar = x_bar, s_sq = s_sq, n = n, g = g)) Welch &lt;- with(summary_stats, Welch_F(x_bar = x_bar, s_sq = s_sq, n = n, g = g)) data.frame(ANOVA = anova_p, Welch = Welch$pval) }, simplify = FALSE) 4.3.2 The BFF* test Write a function that implements the Brown-Forsythe F*-test (the BFF* test!) as described on p. 130 of Brown and Forsythe (1974). Incorporate the function into the calculate_pvals function from the previous question, and use it to estimate rejection rates of the BFF* test for the parameter values in the fifth line of Table 1 (which are the same as those used in the previous question). BF_F &lt;- function(x_bar, s_sq, n, g) { # fill in the guts here return(pval = pval) } # Further R code here "],
["performance-criteria.html", "Chapter 5 Performance criteria 5.1 Parameter estimation: absolute criteria 5.2 Parameter estimation: relative criteria 5.3 Absolute or relative performance? 5.4 Assessing variance estimators 5.5 Hypothesis testing 5.6 Confidence intervals 5.7 Exercises: Simulating Cronbach’s alpha", " Chapter 5 Performance criteria So far, we’ve looked at the structure of simulation studies and seen how to write functions that generate data according to a specified model (and parameters) and functions that implement estimation procedures on simulated data. Put those two together and repeat a bunch of times, and we’ll have a big set of estimates and perhaps also their standard errors and/or confidence intervals. And if the purpose of the simulation is to compare multiple estimation procedures, then we’ll have a set of estimates (SEs, CIs, etc.) for each of the procedures. The question is then: how do we assess the performance of these estimators? In this chapter, we’ll look at a variety of performance criteria that are commonly used to assess how well an estimator works (or to compare the relative performance of multiple estimators). It is important to keep in mind that all of these performance criteria are themselves properties of the sampling distribution of the estimator—that is, assessments of how the estimator behaves if you repeat the experimental process an infinite number of times. We can’t observe this sampling distribution directly (and it can only rarely be worked out in full mathematical detail), but we can sample from it. The set of estimates generated from a simulation constitute a (typically large) sample from the sampling distribution of an estimator. (Say that six times fast!) We can use that sample to estimate the performance criteria of interest. However, because we will work with a sample of replicates rather than the full distribution, we will need to be aware of the precision of the estimates, or what we will call the Monte Carlo standard error (MCSE). Typically, we’ll want to use a large number of replications so that the MCSE is negligible compared to the magnitude of the parameter of interest. Depending on the model and estimation procedures being examined, a range of different criteria might be used to assess estimator performance. The following sections look at four different types of criteria: Absolute estimation criteria, which are most useful for point estimates of location parameters, Relative estimation criteria, which are useful for point estimates of scale parameters and for assessment of the performance of variance estimators, Criteria for hypothesis testing procedures, and Criteria for confidence intervals. 5.1 Parameter estimation: absolute criteria Consider an estimator \\(T\\) for a parameter \\(\\theta\\). A simulation study generates a (typically large) sample of estimators \\(T_1,...,T_K\\). Define the following sample statistics: Sample mean: \\(\\displaystyle{\\bar{T} = \\frac{1}{K}\\sum_{k=1}^K T_k}\\) Sample median: \\(\\displaystyle{\\tilde{T} = T_{(K/2)}}\\) Sample variance: \\(\\displaystyle{S_T^2 = \\frac{1}{K - 1}\\sum_{k=1}^K \\left(T_k - \\bar{T}\\right)^2}\\) Sample skewness (standardized): \\(\\displaystyle{g_T = \\frac{1}{K S_T^3}\\sum_{k=1}^K \\left(T_k - \\bar{T}\\right)^3}\\) Sample kurtosis (standardized): \\(\\displaystyle{k_T = \\frac{1}{K S_T^4} \\sum_{k=1}^K \\left(T_k - \\bar{T}\\right)^4}\\) Commonly used criteria for measuring the performance of \\(T\\) include the bias, variance, and mean-squared error (RMSE) of \\(T\\). Less commonly used criteria include the median bias and the median absolute deviation of \\(T\\). These criteria are defined in the table below. Criterion Definition Estimate MCSE Bias \\(\\text{E}(T) - \\theta\\) \\(\\bar{T} - \\theta\\) \\(\\sqrt{S_T^2/ K}\\) Median bias \\(\\text{M}(T) - \\theta\\) \\(\\tilde{T} - \\theta\\) - Variance \\(\\text{E}\\left[\\left(T - \\text{E}(T)\\right)^2\\right]\\) \\(S_T^2\\) \\(\\displaystyle{S_T^2 \\sqrt{\\frac{k_T - 1}{K}}}\\) MSE \\(\\text{E}\\left[\\left(T - \\theta\\right)^2\\right]\\) \\(\\left(\\bar{T} - \\theta\\right)^2 + S_T^2\\) \\(\\displaystyle{\\sqrt{\\frac{1}{K}\\left[S_T^4 (k_T - 1) + 4 S_T^3 g_T\\left(\\bar{T} - \\theta\\right) + 4 S_T^2 \\left(\\bar{T} - \\theta\\right)^2\\right]}}\\) MAD \\(\\text{M}\\left[\\left|T - \\theta\\right|\\right]\\) \\(\\left[\\left|T - \\theta\\right|\\right]_{K/2}\\) - Bias and median bias are measures of whether the estimator is systematically higher or lower than the target parameter. Variance is a measure of the precision of the estimator—that is, how far it deviates from its average. Mean-squared error (or root mean-squared error) and median absolute deviation are measures of overall accuracy—that is, how far an estimator deviates from the target parameter. For absolute assessments of performance, an estimator with low bias and low MSE is desired. For comparisons of relative performance, an estimator with lower MSE is usually preferable to an estimator with higher MSE; if two estimators have comparable MSE, then the estimator with lower bias (or median bias) would usually be preferable. It is important to recognize that these performance measures depend on the scale of the parameter. For example, if \\(\\theta\\) is the difference in population means on some outcome, then its scale is in the same units as the outcome measure. Standardizing the outcome measure to have unit variance would therefore imply that the bias, median bias, and root mean-squared error are all in standard deviation units. Furthermore, changing the scale of the parameter will lead to changes in magnitude for most of the performance measures. For instance, suppose that \\(\\theta\\) is a measure of the proportion of time that a behavior occurs. A natural way to transform this parameter would be to put it on the log-odds (logit) scale. However, \\[\\text{Bias}\\left[\\text{logit}(T)\\right] \\neq \\text{logit}\\left(\\text{Bias}[T]\\right), \\qquad \\text{MSE}\\left[\\text{logit}(T)\\right] \\neq \\text{logit}\\left(\\text{MSE}[T]\\right),\\] and so on. 5.2 Parameter estimation: relative criteria For parameters that measure scale, or that are always strictly positive, it often makes sense to quantify performance using relative criteria. Relative criteria are very similar to the criteria discussed above, but are defined as proportions of the target parameter, rather than as differences. Consider an estimator \\(L\\) for a parameter \\(\\lambda\\), and define the sample statistics for \\(L\\) analogous to those for \\(T\\). The table below defines several relative performance criteria. Criterion Definition Estimate MCSE Relative bias \\(\\text{E}(L) / \\lambda\\) \\(\\bar{L} / \\lambda\\) \\(\\sqrt{S_L^2 / \\left(K\\lambda^2\\right)}\\) Relative median bias \\(\\text{M}(L) / \\lambda\\) \\(\\tilde{L} / \\lambda\\) - Relative MSE \\(\\text{E}\\left[\\left(L - \\lambda\\right)^2\\right] / \\lambda^2\\) \\(\\frac{\\left(\\bar{L} - \\lambda\\right)^2 + S_L^2}{\\lambda^2}\\) \\(\\displaystyle{\\sqrt{\\frac{1}{K\\lambda^2}\\left[S_L^4 (k_L - 1) + 4 S_L^3 g_L\\left(\\bar{L} - \\lambda\\right) + 4 S_L^2 \\left(\\bar{L} - \\lambda\\right)^2\\right]}}\\) Note that values of 1 for relative bias (and relative median bias) imply that the estimator is exactly unbiased. Both are measures of proportionate under- or over-estimation. For example, a relative bias of 1.12 means that the estimator is, on average, 12% higher than the target parameter. 5.3 Absolute or relative performance? If you are designing a simulation of your own, should you use absolute or relative criteria for assessing estimator performance. So far, I’ve suggested that absolute criteria make sense for location parameters and relative criteria make sense for scale parameters. This is admittedly a very rough heuristic. The argument behind it is that if \\(\\theta\\) is a location parameter, then its bias, variance, and MSE are likely to be (roughly) independent of its magnitude. If that’s true, then re-defining \\(\\theta\\) by adding 1000 (or any arbitrary number) should not change the magnitude of its bias, variance, or MSE. On the other hand, adding 1000 will clearly change the relative bias and relative MSE. (Want smaller relative bias? Just add a million to the parameter!) A more principled approach for determining whether to use absolute or relative performance criteria depends on assessing performance for multiple values of the parameter. In many simulation studies, replications are generated and performance criteria are calculated for several different values of a parameter, say \\(\\theta = \\theta_1,...,\\theta_p\\). Let’s focus on bias for now, and say that we’ve estimated (from a large number of replications) the bias at each parameter value. If the absolute bias is roughly the same for all values of \\(\\theta\\) (as in the plot on the left), then it makes sense to report absolute bias as the summary performance criterion. On the other hand, if the bias grows roughly in proportion to \\(\\theta\\) (as in the plot on the right), then relative bias is a better summary criterion. 5.4 Assessing variance estimators Relative performance criteria are often used to assess the performance of variance estimators (e.g., squared standard errors of regression coefficient estimates) because variance estimators are strictly positive. For example, suppose that the simulation is examining the performance of a point-estimator \\(T\\) for a parameter \\(\\theta\\) and an estimator \\(V\\) for the sampling variance of \\(T\\). In this case, we might not know the true value of the sampling variance of \\(T\\) prior to the simulation. However, we can use the variance of \\(T\\) across the replications (\\(S_T^2\\)) to estimate the true sampling variance \\(\\text{Var}(T)\\). The relative bias would then be estimated by \\(RB = \\bar{V} / S_T^2\\) and the relative MSE would be estimated by \\(\\left(\\bar{V} + S_V^2\\right) / S_T^2\\). Estimating the MCSE of the relative bias or relative MSE is complicated by the appearance of a sample quantity, \\(S_T^2\\), in the denominator of the ratio. To properly assess the overall MCSE, we need to take its uncertainty into account. One way to do so is to use the jackknife technique. Let \\(\\bar{V}_{(j)}\\) and \\(S_{T(j)}^2\\) be statistics calculated from the set of replicates that excludes replicate \\(j\\), for \\(j = 1,...,K\\). The relative bias estimate, excluding replicate \\(j\\) would then be \\(\\bar{V}_{(j)} / S_{T(j)}^2\\). Calculating all \\(K\\) versions of this relative bias estimate and taking the variance yields the jackknife variance estimator: \\[ MCSE\\left(RB\\right) = \\frac{1}{K} \\sum_{j=1}^K \\left(\\frac{\\bar{V}_{(j)}}{S_{T(j)}^2} - \\frac{\\bar{V}}{S_T^2}\\right)^2. \\] This would be quite time-consuming to compute if we did it by brute force. However, with a few algebra tricks we can find a much quicker way. The tricks come from observing that \\[ \\begin{aligned} \\bar{V}_{(j)} &amp;= \\frac{1}{K - 1}\\left(K \\bar{V} - V_j\\right) \\\\ S_{T(j)}^2 &amp;= \\frac{1}{K - 2} \\left[(K - 1) S_T^2 - \\frac{K}{K - 1}\\left(T_j - \\bar{T}\\right)^2\\right] \\end{aligned} \\] These formulas can be used to avoid re-computing the mean and sample variance from every subsample. 5.5 Hypothesis testing In simulations to assess the performance of hypothesis testing procedures, the main performance criterion is the rejection rate of the hypothesis test. When the data are simulated from a model in which the null hypothesis being tested is true, then the rejection rate is equivalent to the Type-I error rate of the test. When the data are simulated from a model in which the null hypothesis is false, then the rejection rate is equivalent to the power of the test (for given, non-null parameter values). Ideally, a testing procedure should have actual Type-I error equal to the nominal level \\(\\alpha\\), but such exact tests are rare. There are some different perspectives on how close the actual Type-I error rate should be in order to qualify as suitable for use in practice. Following a strict statistical definition, a hypothesis testing procedure is said to be level-\\(\\alpha\\) if its actual Type-I error rate is always less than or equal to \\(\\alpha\\). Among a set of level-\\(\\alpha\\) tests, the test with highest power would be preferred. If looking only at null rejection rates, then the test with Type-I error closest to \\(\\alpha\\) would usually be preferred. A less stringent criteria is sometimes used instead, where type I error would be considered acceptable if it is within \\((0.5\\alpha, 1.5 \\alpha)\\). Often, it is of interest to evaluate the performance of the test at several different \\(\\alpha\\) levels. A convenient way to calculate the rejection rate is from a set of simulated \\(p\\)-values. Suppose that \\(P_k\\) is the \\(p\\)-value from simulation replication \\(k\\), for \\(k = 1,...,K\\). Then the rejection rate is for a level-\\(\\alpha\\) test is defined as \\(\\rho_\\alpha = \\text{Pr}\\left(P_k &lt; \\alpha\\right)\\) and estimated as \\[r_\\alpha = \\frac{1}{K} \\sum_{k=1}^K I(P_k &lt; \\alpha).\\] Because the replications are independent, the estimated rejection rate follows a binomial distribution with MCSE \\(\\displaystyle{\\sqrt{\\rho_\\alpha\\left(1 - \\rho_\\alpha\\right) / K}}\\). 5.6 Confidence intervals In simulations to assess the performance of confidence intervals, the main performance criteria are the coverage rate and the average width. Suppose that the confidence intervals are for the target parameter \\(\\theta\\) and have coverage level \\(\\beta\\). Let \\(A_k\\) and \\(B_k\\) denote the lower and upper end-points of the confidence interval from simulation replication \\(k\\), and let \\(W_k = B_k - A_k\\), all for \\(k = 1,...,K\\). The coverage rate and average width criteria are defined in the table below. Criterion Definition Estimate MCSE Coverage \\(\\omega_\\beta = \\text{Pr}(A \\leq \\theta \\leq B)\\) \\(\\frac{1}{K}\\sum_{k=1}^K I(A_k \\leq \\theta \\leq B_k)\\) \\(\\sqrt{\\omega_\\beta \\left(1 - \\omega_\\beta\\right) / K}\\) Expected width \\(\\text{E}(W) = \\text{E}(B - A)\\) \\(\\bar{W} = \\bar{B} - \\bar{A}\\) \\(\\sqrt{S_W^2 / K}\\) Just as with hypothesis testing, a strict statistical interpretation would deem a hypothesis testing procedure acceptable if it has actual coverage rate greater than or equal to \\(\\beta\\). If multiple tests satisfy this criterion, then the test with the lowest expected width would be preferable. Some analysts prefer to look at lower and upper coverage separately, where lower coverage is \\(\\text{Pr}(A \\leq \\theta)\\) and upper coverage is \\(\\text{Pr}(\\theta \\leq B)\\). 5.7 Exercises: Simulating Cronbach’s alpha Cronbach’s \\(\\alpha\\) coefficient is commonly reported as a measure of the internal consistency among a set of test items. Consider a set of \\(p\\) test items with population variance-covariance matrix \\(\\boldsymbol\\Phi = \\left[\\phi_{ij}\\right]_{i,j=1}^p\\). The \\(\\alpha\\) parameter is then defined as \\[ \\alpha = \\frac{p}{p - 1}\\left(1 - \\frac{\\sum_{i=1}^p \\phi_{ii}}{\\sum_{i=1}^p \\sum_{j=1}^p \\phi_{ij}}\\right). \\] Given a sample of size \\(n\\), the usual estimate of \\(\\alpha\\) is obtained by replacing the population variances and covariances with corresponding sample estimates. Letting \\(s_{ij}\\) denote the sample covariance of items \\(i\\) and \\(j\\) \\[ A = \\frac{p}{p - 1}\\left(1 - \\frac{\\sum_{i=1}^p s_{ii}}{\\sum_{i=1}^p \\sum_{j=1}^p s_{ij}}\\right). \\] If we assume that the items follow a multivariate normal distribution, then \\(A\\) corresponds to the maximum likelihood estimator of \\(\\alpha\\). In these exercises, we will examine the properties of this estimator when the set of \\(P\\) items is not multi-variate normal, but rather follows a multivariate t distribution with \\(v\\) degrees of freedom. For simplicity, we shall assume that the items have common variance and have a compound symmetric covariance matrix, such that \\(\\phi_{11} = \\phi_{22} = \\cdots = \\phi_{pp} = \\phi\\) and \\(\\phi_{ij} = \\rho \\phi\\). In this case, \\[ \\alpha = \\frac{p \\rho}{1 + \\rho (p - 1)}. \\] 5.7.1 data-generating function The following function generates a sample of \\(n\\) observations of \\(p\\) items from a multivariate t distribution with a compound symmetric covariance matrix, intra-class correlation \\(\\rho\\), and \\(v\\) degrees of freedom: library(mvtnorm) r_mvt_items &lt;- function(n, p, icc, df) { V_mat &lt;- icc + diag(1 - icc, nrow = p) X &lt;- rmvt(n = n, sigma = V_mat, df = df) colnames(X) &lt;- LETTERS[1:p] X } small_sample &lt;- r_mvt_items(n = 8, p = 3, icc = 0.7, df = 5) small_sample ## A B C ## [1,] 0.25083301 0.1464913 0.10553683 ## [2,] 0.10078588 -0.2478199 -0.31257997 ## [3,] -0.06477356 -0.4959787 0.92095484 ## [4,] 2.97552595 1.1585854 2.17694682 ## [5,] 0.63664212 -0.7712058 0.57794704 ## [6,] 1.76182445 1.2307978 1.83414729 ## [7,] -0.13057774 -1.1214898 -0.03541368 ## [8,] -0.33005904 0.1906131 0.13196280 To check that the function is indeed simulating data following the intended distribution, let’s generate a very large sample of items. We can then verify that the correlation matrix of the items is compound-symmetric and that the marginal distributions of the items follow t distributions with specified degrees of freedom. big_sample &lt;- r_mvt_items(n = 100000, p = 4, icc = 0.7, df = 5) round(cor(big_sample), 3) # looks good ## A B C D ## A 1.000 0.705 0.704 0.706 ## B 0.705 1.000 0.702 0.704 ## C 0.704 0.702 1.000 0.705 ## D 0.706 0.704 0.705 1.000 qqplot(qt(ppoints(200), df = 5), big_sample[,2], ylim = c(-4,4)) 5.7.2 Estimation function van Zyl, Neudecker, and Nel (2000) demonstrate that, if the items have a compound-symmetric covariance matrix, then the asymptotic variance of \\(A\\) is \\[ \\text{Var}(A) \\approx \\frac{2p(1 - \\alpha)^2}{(p - 1) n}. \\] Substituting \\(A\\) in place of \\(\\alpha\\) gives an estimate of the variance of \\(A\\). The following function calculates \\(A\\) and its variance estimator from a sample of data: estimate_alpha &lt;- function(dat) { V &lt;- cov(dat) p &lt;- ncol(dat) n &lt;- nrow(dat) A &lt;- p / (p - 1) * (1 - sum(diag(V)) / sum(V)) Var_A &lt;- 2 * p * (1 - A)^2 / ((p - 1) * n) data.frame(A = A, Var = Var_A) } estimate_alpha(small_sample) ## A Var ## 1 0.9142744 0.002755826 The psych package provides a function for calculating \\(\\alpha\\), which can be used to verify that the calculation of \\(A\\) in estimate_alpha is correct: library(psych) ## ## Attaching package: &#39;psych&#39; ## The following objects are masked from &#39;package:ggplot2&#39;: ## ## %+%, alpha summary(alpha(x = small_sample))$raw_alpha ## ## Reliability analysis ## raw_alpha std.alpha G6(smc) average_r S/N ase mean sd ## 0.91 0.92 0.9 0.79 12 0.048 0.45 0.9 ## [1] 0.9142744 5.7.3 Replicates The following function generates a specified number of replicates of \\(A\\) and its variance estimator, for user-specified parameter values \\(n\\), \\(p\\), \\(\\alpha\\), and \\(v\\): library(dplyr) simulate_alpha &lt;- function(reps, n, p, alpha, df) { icc &lt;- alpha / (p - alpha * (p - 1)) replicate(reps, { dat &lt;- r_mvt_items(n = n, p = p, icc = icc, df = df) estimate_alpha(dat) }, simplify = FALSE) %&gt;% bind_rows() } We can use it to generate 1000 replicates using samples of size \\(n = 40\\), \\(p = 6\\) items, a true \\(\\alpha = 0.8\\), and \\(v = 5\\) degrees of freedom: reps &lt;- 1000 alpha_true &lt;- 0.8 alpha_reps &lt;- simulate_alpha(reps = reps, n = 40, p = 6, alpha = alpha_true, df = 5) head(alpha_reps) ## A Var ## 1 0.8331526 0.0016702826 ## 2 0.8478141 0.0013896333 ## 3 0.8166545 0.0020169335 ## 4 0.7948754 0.0025245661 ## 5 0.7844087 0.0027887765 ## 6 0.8736379 0.0009580435 5.7.4 Estimator performance With the parameters specified above, calculate the bias of \\(A\\). Also calculate the Monte Carlo standard error (MCSE) of the bias estimate. Calculate the mean squared error of \\(A\\), along with its MCSE. Calculate the relative bias of the asymptotic variance estimator. (Challenge problem) Use the jackknife to calculate the MCSE for the relative bias of the asymptotic variance estimator. (Challenge problem) Modify the simulate_alpha function so that it returns a one-row data frame with columns corresponding to the bias, mean squared error, and relative bias of the asymptotic variance estimator. Use the function to evaluate the performance of \\(A\\) for true values of \\(\\alpha\\) ranging from 0.5 to 0.9 (i.e., alpha_true_seq &lt;- seq(0.5, 0.9, 0.1)). 5.7.5 Confidence interval coverage One way to obtain an approximate confidence interval for \\(\\alpha\\) would be to take \\(A \\pm z \\sqrt{\\text{Var}(A)}\\), where \\(\\text{Var}(A)\\) is estimated as described above and \\(z\\) is a standard normal critical value at the appropriate level (i.e., \\(z = 1.96\\) for a 95% CI). However, van Zyl, Neudecker, and Nel (2000) suggest that a better approximation involves first applying a transformation to \\(A\\), then calculating a confidence interval, then back-transforming to the original scale (this is very similar to the procedure for calculating confidence intervals for correlation coefficients, using Fisher’s \\(z\\) transformation). Let \\[ \\begin{aligned} \\beta &amp;= \\frac{1}{2} \\ln\\left(1 - \\alpha\\right) \\\\ B &amp;= \\frac{1}{2} \\ln\\left(1 - A\\right) \\end{aligned} \\] and \\[ V^B = \\frac{p}{2 n (p - 1)}. \\] An approximate confidence interval for \\(\\beta\\) is given by \\([B_L, B_U]\\), where \\[ B_L = B - z \\sqrt{V^B}, \\qquad B_U = B + z \\sqrt{V^B}. \\] Applying the inverse of the transformation gives a confidence interval for \\(\\alpha\\): \\[ \\left[1 - \\exp(2B_U), \\ 1 - \\exp(2 B_L)\\right]. \\] Modify the estimate_alpha function to calculate a confidence interval for \\(\\alpha\\), following the method described above. With the modified version of estimate_alpha, re-run the following code to obtain 1000 replicated confidence intervals. Calculate the true coverage rate of the confidence interval. Also calculate the Monte Carlo standard error (MCSE) of this coverage rate. reps &lt;- 1000 alpha_true &lt;- 0.8 alpha_reps &lt;- simulate_alpha(reps = reps, n = 40, p = 6, alpha = alpha_true, df = 5) Calculate the average width of the confidence interval for \\(\\alpha\\), along with its MCSE. "],
["experimental-design-1.html", "Chapter 6 Experimental design 6.1 Choosing parameter combinations", " Chapter 6 Experimental design So far, we’ve created code that will give us results for a single combination of parameter values. In practice, simulation studies typically examine a range of different values, including varying the level of the true parameter values and perhaps also varying sample sizes. Let’s now look at the remaining piece of the simulation puzzle: the study’s experimental design. Simulation studies often take the form of full factorial designed experiments. In full factorials, each factor is varied across multiple levels, and the design includes every possible combination of the levels of every factor. One way to represent such a design is as a list of factors and levels. For the Cronbach alpha simulation, we might want to vary the true value of alpha, with values ranging from 0.1 to 0.9; the degrees of freedom of the multivariate t distribution, with values of 5, 10, 20, or 100; the sample size, with values of 50 or 100; and the number of items, with values of 4 or 8. Here is code that implements this design, using 500 replications per condition: set.seed(20170405) # now express the simulation parameters as vectors/lists design_factors &lt;- list( n = c(50, 100), p = c(4, 8), alpha = seq(0.1, 0.9, 0.1), df = c(5, 10, 20, 100) ) params &lt;- expand.grid(design_factors) params$iterations &lt;- 50 params$seed &lt;- round(runif(1) * 2^30) + 1:nrow(params) This gives us a \\(2\\times2\\times9\\times4\\) factorial design: lengths(design_factors) ## n p alpha df ## 2 2 9 4 With a total of 144 cells. nrow(params) ## [1] 144 The params data frame is a representation of the full experimental design: head(params) ## n p alpha df iterations seed ## 1 50 4 0.1 5 50 1047992101 ## 2 100 4 0.1 5 50 1047992102 ## 3 50 8 0.1 5 50 1047992103 ## 4 100 8 0.1 5 50 1047992104 ## 5 50 4 0.2 5 50 1047992105 ## 6 100 4 0.2 5 50 1047992106 6.1 Choosing parameter combinations We’ve now seen how to create a set of experimental conditions, but how do we go about choosing parameter values to examine? Choosing parameters is a central part of good simulation design because the primary limitation of simulation studies is always their generalizability. On the one hand, it’s difficult to extrapolate findings from a simulation study beyond the set of simulation conditions that were examined. On the other hand, it’s often difficult or impossible to examine the full space of all possible parameter values, except for very simple problems. Even in the Cronbach alpha simulation, we’ve got four factors, and the last three could each take an infinite number of different levels, in theory. How can we come up with a defensible set of levels to examine? The choice of simulation conditions needs to be made in the context of the problem or model that you’re studying, so it’s a bit difficult to offer valid, decontextualized advice. I’ll offer a couple of observations all the same: For research simulations, it often is important to be able to relate your findings to previous research. This suggests that you should select parameter levels to make this possible, such as by looking at sample sizes similar to those examined in previous studies. That said, previous simulation studies are not always perfect (actually, there’s a lot of really crummy ones out there!), and so this shouldn’t be your sole guide or justification. Generally, I think it is better to err on the side of being more comprehensive. You learn more by looking at a broader range of conditions, and you can always boil down your results to a more limited set of conditions for purposes of presentation. I also think it is important to explore breakdown points (e.g., what sample size is too small for a method to work?) rather than focusing only on conditions where a method might be expected to work well. Pushing the boundaries and identifying conditions where estimation methods break will help you to provide better guidance for how the methods should be used in practice. "],
["putting-the-pieces-together.html", "Chapter 7 Putting the pieces together 7.1 Organizing the script for a simulation study 7.2 data-generating model 7.3 estimation procedures 7.4 Performance calculations 7.5 Simulation driver 7.6 Experimental design 7.7 Putting it all together", " Chapter 7 Putting the pieces together Earlier, I suggested that we can think of simulation studies as having five main components: I also proposed that the code we write to implement simulations should follow the same structure, with different functions corresponding to each component. So far, we’ve looked at the middle three components: data-generating model estimation methods performance criteria In these notes, we’ll start to fill out the remainder, by looking at the overall organization of code for a simulation study. We’ll use as a running example the simulation of Cronbach’s \\(\\alpha\\) coefficient, which we saw in Class 17. 7.1 Organizing the script for a simulation study In my methodological work, I try to always follow the same workflow when writing simulations. I make no claim that this is the only or best way to do things—only that it works for me. You see my template by doing the following: Run the following code to install my personal package of helper functions: library(devtools) install_github(&quot;jepusto/Pusto&quot;) Load the Pusto library and run the Simulation_Skeleton() function as follows: library(Pusto) Simulation_Skeleton(&quot;Cronbach Alpha simulation&quot;) This function will open up a new R script for you, called &quot;Cronbach Alpha simulation.R&quot;, which contains a template for a simulation study, with sections corresponding to each component. 7.2 data-generating model The first two sections are about the data-generating model: rm(list = ls()) #------------------------------------------------------ # Set development values for simulation parameters #------------------------------------------------------ # What are your model parameters? # What are your design parameters? #------------------------------------------------------ # Data Generating Model #------------------------------------------------------ dgm &lt;- function(model_params) { return(dat) } # Test the data-generating model - How can you verify that it is correct? Here, we need to create and test a function that takes model parameters (and sample sizes and such) as inputs, and produces a simulated dataset. For the Cronbach alpha simulation, the function looks like this: library(mvtnorm) rm(list = ls()) #------------------------------------------------------ # Set development values for simulation parameters #------------------------------------------------------ # model parameters alpha &lt;- 0.73 # true alpha df &lt;- 12 # degrees of freedom # design parameters n &lt;- 50 # sample size p &lt;- 6 # number of items #------------------------------------------------------ # Data Generating Model #------------------------------------------------------ r_mvt_items &lt;- function(n, p, alpha, df) { icc &lt;- alpha / (p - alpha * (p - 1)) V_mat &lt;- icc + diag(1 - icc, nrow = p) X &lt;- rmvt(n = n, sigma = V_mat, df = df) colnames(X) &lt;- LETTERS[1:p] X } # Test the data-generating model big_sample &lt;- r_mvt_items(n = 100000, p = 4, alpha = 0.73, df = 5) round(cor(big_sample), 3) # looks good ## A B C D ## A 1.000 0.402 0.411 0.399 ## B 0.402 1.000 0.402 0.407 ## C 0.411 0.402 1.000 0.397 ## D 0.399 0.407 0.397 1.000 qqplot(qt(ppoints(200), df = 5), big_sample[,2], ylim = c(-4,4)) 7.3 estimation procedures The next section of the template looks like this: #------------------------------------------------------ # Model-fitting/estimation/testing functions #------------------------------------------------------ estimate &lt;- function(dat, design_params) { return(result) } # Test the estimation function Here, we need to create a function that takes simulated data as input (and possibly also design parameters, like sample size), and produces a set of estimates (or confidence intervals, or p-values, etc.). As usual, we should also test that the function is correct. Here’s what this code looks like for the Cronbach alpha simulation: #------------------------------------------------------ # Model-fitting/estimation/testing functions #------------------------------------------------------ estimate_alpha &lt;- function(dat, coverage = .95) { V &lt;- cov(dat) p &lt;- ncol(dat) n &lt;- nrow(dat) A &lt;- p / (p - 1) * (1 - sum(diag(V)) / sum(V)) Var_A &lt;- 2 * p * (1 - A)^2 / ((p - 1) * n) B &lt;- log(1 - A) / 2 SE_B &lt;- sqrt(p / (2 * n * (p - 1))) z &lt;- qnorm((1 - coverage) / 2) CI_B &lt;- B + c(-1, 1) * SE_B * z CI_A &lt;- 1 - exp(2 * CI_B) data.frame(A = A, Var_A = Var_A, CI_L = CI_A[1], CI_U = CI_A[2]) } # Test the estimation function small_sample &lt;- r_mvt_items(n = 50, p = 6, alpha = 0.73, df = 5) estimate_alpha(small_sample) ## A Var_A CI_L CI_U ## 1 0.6322428 0.006491778 0.4349978 0.7606286 The function takes a simulated dataset as input and spits out a point estimate of alpha, an estimate of the variance of alpha, and a confidence interval for alpha (at the 95% coverage level, by default). We’ve already seen how to use the replicate function to generate a whole bunch of simulated estimates: alpha_sims &lt;- replicate(n = 10, { dat &lt;- r_mvt_items(n = 50, p = 6, alpha = 0.73, df = 5) estimate_alpha(dat) }, simplify = FALSE) %&gt;% bind_rows() alpha_sims ## A Var_A CI_L CI_U ## 1 0.7841786 0.002235786 0.6684237 0.8595229 ## 2 0.6318808 0.006504564 0.4344417 0.7603930 ## 3 0.7156552 0.003880895 0.5631481 0.8149213 ## 4 0.7436121 0.003155269 0.6060995 0.8331183 ## 5 0.7138774 0.003929575 0.5604168 0.8137642 ## 6 0.7666345 0.002614053 0.6414700 0.8481035 ## 7 0.8464022 0.001132430 0.7640207 0.9000239 ## 8 0.7614484 0.002731529 0.6335023 0.8447279 ## 9 0.6262690 0.006704395 0.4258200 0.7567402 ## 10 0.7805177 0.002312279 0.6627993 0.8571400 7.4 Performance calculations The next section of the template deals with performance calculations: #------------------------------------------------------ # Calculate performance measures # (For some simulations, it may make more sense # to do this as part of the simulation driver.) #------------------------------------------------------ performance &lt;- function(results, model_params) { return(performance_measures) } # Check performance calculations The performance() function takes as input a bunch of simulated data (which we might call results) and the true values of the model parameters (model_params) and returns as output a set of summary performance measures. As noted in the comments above, for simple simulations it might not be necessary to write a separate function to do these calculations. For more complex simulations, though, it can be helpful to break these calculations out in a function. For the Cronbach alpha simulation, we might want to calculate the following performance measures: bias and root mean-squared error (RMSE) of the alpha point estimate relative bias of the variance estimator coverage of the confidence interval Here is a function that calculates these measures (along with Monte Carlo standard errors), given a data frame containing model results. It uses the jackknife technique to get Monte Carlo standard errors for the RMSE and relative bias. #------------------------------------------------------ # Calculate performance measures #------------------------------------------------------ alpha_performance &lt;- function(alpha_sims, alpha, coverage_level = .95) { # setup K &lt;- nrow(alpha_sims) A_err &lt;- alpha_sims$A - alpha var_A &lt;- var(alpha_sims$A) # bias A_bias &lt;- mean(A_err) A_bias_MCSE &lt;- sqrt(var_A / K) # RMSE A_RMSE &lt;- sqrt(mean((A_err)^2)) RMSE_j &lt;- sqrt((A_RMSE^2 * K - A_err^2) / (K - 1)) A_RMSE_MCSE &lt;- sd(RMSE_j) # relative bias of variance estimator V_bar &lt;- mean(alpha_sims$Var_A) V_j &lt;- (V_bar * K - alpha_sims$Var_A) / (K - 1) Ssq_j &lt;- ((K - 1) * var_A - A_err^2 * K / (K - 1)) / (K - 2) RB_j &lt;- V_j / Ssq_j V_relbias &lt;- V_bar / var_A V_relbias_MCSE &lt;- sd(RB_j) # coverage coverage &lt;- mean(alpha_sims$CI_L &lt; alpha &amp; alpha &lt; alpha_sims$CI_U) coverage_MCSE &lt;- sqrt(coverage_level * (1 - coverage_level) / K) data.frame( criterion = c(&quot;alpha bias&quot;,&quot;alpha RMSE&quot;, &quot;V relative bias&quot;, &quot;coverage&quot;), est = c(A_bias, A_RMSE, V_relbias, coverage), MCSE = c(A_bias_MCSE, A_RMSE_MCSE, V_relbias_MCSE, coverage_MCSE) ) } # Check performance calculations alpha_performance(alpha_sims, alpha = 0.73) ## criterion est MCSE ## 1 alpha bias 0.00704758 0.021579244 ## 2 alpha RMSE 0.06512021 0.004456856 ## 3 V relative bias 0.75592686 0.138056334 ## 4 coverage 0.90000000 0.068920244 7.5 Simulation driver We now have all the components we need to get simulation results, given a set of parameter values. In the next section of the template, we put all these pieces together in a function—which we might call the simulation driver—that takes as input 1) parameter values, 2) the desired number of replications, and 3) optionally, a seed value. The function produces as output a single set of performance estimates. Generically, the function looks like this: #----------------------------------------------------------- # Simulation Driver - should return a data.frame or tibble #----------------------------------------------------------- runSim &lt;- function(iterations, model_params, design_params, seed = NULL) { if (!is.null(seed)) set.seed(seed) results &lt;- replicate(iterations, { dat &lt;- dgm(model_params) estimate(dat, design_params) }) performance(results, model_params) } # demonstrate the simulation driver The runSim function should require very little modification for a new simulation. Essentially, all we need to change is the names of the functions that are called, so that they line up with the functions we have designed for our simulation. Here’s what this looks like for the Cronbach alpha simulation: #----------------------------------------------------------- # Simulation Driver - should return a data.frame or tibble #----------------------------------------------------------- run_alpha_sim &lt;- function(iterations, n, p, alpha, df, coverage = 0.95, seed = NULL) { if (!is.null(seed)) set.seed(seed) results &lt;- replicate(n = iterations, { dat &lt;- r_mvt_items(n = n, p = p, alpha = alpha, df = df) estimate_alpha(dat, coverage = coverage) }, simplify = FALSE) %&gt;% bind_rows() alpha_performance(results, alpha = alpha, coverage = coverage) } # demonstrate the simulation driver run_alpha_sim(iterations = 10, n = 50, p = 6, alpha = 0.73, df = 5) ## criterion est MCSE ## 1 alpha bias -0.006834839 0.043602710 ## 2 alpha RMSE 0.130986570 0.008398788 ## 3 V relative bias 0.236688771 0.024706868 ## 4 coverage 0.600000000 0.068920244 Because this function involves generating random numbers, re-running it with the exact same input parameters will still produce different results: run_alpha_sim(iterations = 10, n = 50, p = 6, alpha = 0.73, df = 5) ## criterion est MCSE ## 1 alpha bias -0.01454455 0.029200507 ## 2 alpha RMSE 0.08880073 0.006142837 ## 3 V relative bias 0.49898603 0.061950577 ## 4 coverage 0.80000000 0.068920244 Of course, using a larger number of iterations will give us more precise estimates of the performance criteria. If we want to get the exact same results, we can feed the function a seed value: run_alpha_sim(iterations = 10, n = 50, p = 6, alpha = 0.73, df = 5, seed = 6) ## criterion est MCSE ## 1 alpha bias -0.02053560 0.02585963 ## 2 alpha RMSE 0.08025083 0.01344827 ## 3 V relative bias 0.64909209 1.43035647 ## 4 coverage 0.90000000 0.06892024 run_alpha_sim(iterations = 10, n = 50, p = 6, alpha = 0.73, df = 5, seed = 6) ## criterion est MCSE ## 1 alpha bias -0.02053560 0.02585963 ## 2 alpha RMSE 0.08025083 0.01344827 ## 3 V relative bias 0.64909209 1.43035647 ## 4 coverage 0.90000000 0.06892024 This is useful because it ensure the full reproducibility of the results. In practice, it is a good idea to always set seed values for your simulations, so that you (or someone else!) can exactly reproduce the results. 7.6 Experimental design The next section of the template looks like this: set.seed(20150316) # change this seed value! # now express the simulation parameters as vectors/lists design_factors &lt;- list(factor1 = , factor2 = , ...) # combine into a design set params &lt;- expand.grid(design_factors) params$iterations &lt;- 5 params$seed &lt;- round(runif(1) * 2^30) + 1:nrow(params) # All look right? lengths(design_factors) nrow(params) head(params) For the Cronbach alpha simulation, we might want to vary the true value of alpha, with values ranging from 0.1 to 0.9; the degrees of freedom of the multivariate t distribution, with values of 5, 10, 20, or 100; the sample size, with values of 50 or 100; and the number of items, with values of 4 or 8. Here is code that implements this design, using 500 replications per condition: set.seed(20170405) # now express the simulation parameters as vectors/lists design_factors &lt;- list( n = c(50, 100), p = c(4, 8), alpha = seq(0.1, 0.9, 0.1), df = c(5, 10, 20, 100) ) params &lt;- expand.grid(design_factors) params$iterations &lt;- 50 params$seed &lt;- round(runif(1) * 2^30) + 1:nrow(params) This gives us a \\(2\\times2\\times9\\times4\\) factorial design: lengths(design_factors) ## n p alpha df ## 2 2 9 4 With a total of 144 cells. nrow(params) ## [1] 144 The params data frame is a representation of the full experimental design: head(params) ## n p alpha df iterations seed ## 1 50 4 0.1 5 50 1047992101 ## 2 100 4 0.1 5 50 1047992102 ## 3 50 8 0.1 5 50 1047992103 ## 4 100 8 0.1 5 50 1047992104 ## 5 50 4 0.2 5 50 1047992105 ## 6 100 4 0.2 5 50 1047992106 7.7 Putting it all together In the previous sections, we’ve created code that will generate a set of performance estimates, given a set of parameter values. We’ve also created a dataset that represents every combination of parameter values that we want to examine. How do we put the pieces together? If we only had a couple of parameter combinations, it would be easy enough to just call our run_alpha_sim function a couple of times: run_alpha_sim(iterations = 100, n = 50, p = 4, alpha = 0.7, df = 5) ## criterion est MCSE ## 1 alpha bias -0.03641974 0.0099773396 ## 2 alpha RMSE 0.10574299 0.0009324136 ## 3 V relative bias 0.65916226 0.0108366286 ## 4 coverage 0.84000000 0.0217944947 run_alpha_sim(iterations = 100, n = 100, p = 4, alpha = 0.7, df = 5) ## criterion est MCSE ## 1 alpha bias -0.01086245 0.0076446904 ## 2 alpha RMSE 0.07683541 0.0007246681 ## 3 V relative bias 0.46734606 0.0082316474 ## 4 coverage 0.82000000 0.0217944947 run_alpha_sim(iterations = 100, n = 50, p = 8, alpha = 0.7, df = 5) ## criterion est MCSE ## 1 alpha bias -0.03251583 0.01051087 ## 2 alpha RMSE 0.10952007 0.00138845 ## 3 V relative bias 0.50276550 0.01258413 ## 4 coverage 0.84000000 0.02179449 run_alpha_sim(iterations = 100, n = 100, p = 8, alpha = 0.7, df = 5) ## criterion est MCSE ## 1 alpha bias -0.01917588 0.007923788 ## 2 alpha RMSE 0.08113920 0.001070873 ## 3 V relative bias 0.39349401 0.010781848 ## 4 coverage 0.85000000 0.021794495 But the simulation that we’ve designed has 144 cells—too many to do this “by hand.” The next two sections of the simulation template demonstrate two different approaches to doing the calculations for every combination of parameter values. You’ll only need to use one of these approaches, so pick whichever you find easier. 7.7.1 mdply workflow The first approach uses the mdply function from the plyr package: #-------------------------------------------------------- # run simulations in serial - mdply workflow #-------------------------------------------------------- system.time(results &lt;- plyr::mdply(params, .fun = runSim)) The main function here is mdply. I call it by specifying the package name first, followed by two colons, followed by the function name (this avoids the need to load the plyr package, which has a lot of conflicts with other packages such as dplyr and tidyr). The results are stored in an object called results. And the whole line is wrapped in a call to the system.time function, so that we’ll know how long the full set of calculations takes. Here’s the syntax for the Cronbach alpha simulation: #-------------------------------------------------------- # run simulations in serial - mdply workflow #-------------------------------------------------------- system.time(results_mdply &lt;- plyr::mdply(params, .fun = run_alpha_sim)) ## user system elapsed ## 7.862 0.001 7.862 The output is then as follows: head(results_mdply, n = 12) ## n p alpha df iterations seed criterion est ## 1 50 4 0.1 5 50 1047992101 alpha bias -0.08538795 ## 2 50 4 0.1 5 50 1047992101 alpha RMSE 0.31148392 ## 3 50 4 0.1 5 50 1047992101 V relative bias 0.61784942 ## 4 50 4 0.1 5 50 1047992101 coverage 0.90000000 ## 5 100 4 0.1 5 50 1047992102 alpha bias -0.06421672 ## 6 100 4 0.1 5 50 1047992102 alpha RMSE 0.22382778 ## 7 100 4 0.1 5 50 1047992102 V relative bias 0.55460480 ## 8 100 4 0.1 5 50 1047992102 coverage 0.84000000 ## 9 50 8 0.1 5 50 1047992103 alpha bias -0.01166624 ## 10 50 8 0.1 5 50 1047992103 alpha RMSE 0.24503102 ## 11 50 8 0.1 5 50 1047992103 V relative bias 0.66637452 ## 12 50 8 0.1 5 50 1047992103 coverage 0.84000000 ## MCSE ## 1 0.042793079 ## 2 0.008572716 ## 3 0.040803129 ## 4 0.030822070 ## 5 0.030631146 ## 6 0.003958238 ## 7 0.020183400 ## 8 0.030822070 ## 9 0.034964735 ## 10 0.004031325 ## 11 0.020205867 ## 12 0.030822070 What’s going on here? The mdply function takes two main arguments: a data frame and a function. The specified function is then evaluated for each row of the data frame. The arguments to the function are matched to the variable names in the dataset, so the params$alpha is matched to the alpha argument of run_alpha_sim, params$df is matched to the df argument of run_alpha_sim, etc. Thus, it is crucial that the variable names in the params dataset exactly match the argument names of the simulation driver. Additional arguments can also be specified after the function name; these will be used for every row of the dataset. For example: plyr::mdply(params, .fun = run_alpha_sim, coverage = 0.90) Would calculate the coverage of 90% confidence intervals instead of the default 95% CIs. 7.7.2 purrrlyr workflow An alternative to mdply is to use the invoke_rows function from the purrrlyr package. The functions work almost identically, with a few little differences. One difference is that the purrrlyr package is designed to work well with tidyr, dplyr and the other tidyverse packages. We’ll see another difference below. The simulation template includes the following code to execute the simulations using invoke_rows: #-------------------------------------------------------- # run simulations in serial - purrrlyr workflow #-------------------------------------------------------- library(purrrlyr) system.time( results &lt;- params %&gt;% invoke_rows(.f = runSim, .to = &quot;res&quot;) ) The invoke_rows function takes two main arguments, just like mdply: the dataset and the function to evaluate. The function is called for each row of the dataset, again by matching the function arguments to the variable names. The result is stored in a new variable, with name specified by .to. Here’s the syntax for the Cronbach alpha simulation: #-------------------------------------------------------- # run simulations in serial - purrrlyr workflow #-------------------------------------------------------- library(purrrlyr) system.time( results_purrr &lt;- params %&gt;% invoke_rows(.f = run_alpha_sim, .to = &quot;res&quot;) ) ## user system elapsed ## 7.803 0.000 7.803 The result looks a bit different than the result of using mdply: head(results_purrr, n = 12) ## # A tibble: 12 x 7 ## n p alpha df iterations seed res ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;list&gt; ## 1 50 4 0.1 5 50 1047992101 &lt;data.frame [4 x 3]&gt; ## 2 100 4 0.1 5 50 1047992102 &lt;data.frame [4 x 3]&gt; ## 3 50 8 0.1 5 50 1047992103 &lt;data.frame [4 x 3]&gt; ## 4 100 8 0.1 5 50 1047992104 &lt;data.frame [4 x 3]&gt; ## 5 50 4 0.2 5 50 1047992105 &lt;data.frame [4 x 3]&gt; ## 6 100 4 0.2 5 50 1047992106 &lt;data.frame [4 x 3]&gt; ## 7 50 8 0.2 5 50 1047992107 &lt;data.frame [4 x 3]&gt; ## 8 100 8 0.2 5 50 1047992108 &lt;data.frame [4 x 3]&gt; ## 9 50 4 0.3 5 50 1047992109 &lt;data.frame [4 x 3]&gt; ## 10 100 4 0.3 5 50 1047992110 &lt;data.frame [4 x 3]&gt; ## 11 50 8 0.3 5 50 1047992111 &lt;data.frame [4 x 3]&gt; ## 12 100 8 0.3 5 50 1047992112 &lt;data.frame [4 x 3]&gt; The output has the same number of rows as params (compare to results_mdply, which has 576). The performance estimates are all in a single variable, called res in this case, and each observation is actually its own little dataset, or what is called a nested data frame. Nested data frames are nifty but odd little data structures, which are beyond the scope of this course. For now, we can turn the results into something easier to work with by using the unnest function from tidyr: library(tidyr) results_purrr %&gt;% unnest() %&gt;% head(n = 12) ## # A tibble: 12 x 9 ## n p alpha df iterations seed criterion ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fctr&gt; ## 1 50 4 0.1 5 50 1047992101 alpha bias ## 2 50 4 0.1 5 50 1047992101 alpha RMSE ## 3 50 4 0.1 5 50 1047992101 V relative bias ## 4 50 4 0.1 5 50 1047992101 coverage ## 5 100 4 0.1 5 50 1047992102 alpha bias ## 6 100 4 0.1 5 50 1047992102 alpha RMSE ## 7 100 4 0.1 5 50 1047992102 V relative bias ## 8 100 4 0.1 5 50 1047992102 coverage ## 9 50 8 0.1 5 50 1047992103 alpha bias ## 10 50 8 0.1 5 50 1047992103 alpha RMSE ## 11 50 8 0.1 5 50 1047992103 V relative bias ## 12 50 8 0.1 5 50 1047992103 coverage ## # ... with 2 more variables: est &lt;dbl&gt;, MCSE &lt;dbl&gt; 7.7.3 parallel processing The final two sections of the simulation template demonstrate how to execute the simulations in parallel, across multiple cores of a computer or computing cluster. We’ll look at the details here in a later class. "],
["design-analysis-and-presentation-of-simulation-results.html", "Chapter 8 Design, analysis, and presentation of simulation results 8.1 Designing the simulation experiment 8.2 Choosing parameter levels 8.3 Presentation 8.4 Tabulation 8.5 Visualization 8.6 Modeling 8.7 Presentation", " Chapter 8 Design, analysis, and presentation of simulation results 8.1 Designing the simulation experiment The primary limitation of simulation studies is generalizability. Choose conditions that allow you to relate findings to previous work. Err towards being comprehensive. The goal should be to build an understanding of the major moving parts. Presentation of results can always be tailored to illustrate trends. Explore breakdown points (e.g., what sample size is too small for applying a given method?). 8.2 Choosing parameter levels Four possible strategies: Vary a parameter over its entire range (or nearly so). Choose parameter levels to represent realistic practical range. Empirical justification based on systematic reviews of applications Or at least informal impressions of what’s realistic in practice Vary nuisance parameters (at least a little) to test sensitivity of results. Choose parameters to emulate one important application. 8.3 Presentation Your results have finished running…what now? Understand the effects of all of the factors manipulated in the simulation Develop evidence that addresses your research questions Three approaches to analysis and presentation: Tabulation Visualization Modeling 8.4 Tabulation Traditionally, simulation study results are presented in big tables. Tables are fine if… they involve only a few numbers, and a few targeted comparisons it is important to report exact values for some quantities But simulations usually produce lots of numbers, and involve making lots of comparisons. relative performance of alternative estimators performance under different conditions for the data-generating model Exact values for bias/RMSE/type-I error are not usually of interest. It is often more useful and insightful to present results in graphs (Gelman, Pasarica, &amp; Dodhia, 2002). 8.5 Visualization Visualization should nearly always be the first step in analyzing simulation results. This often requires creating a BUNCH of graphs to look at different aspects of the data. Helpful tools/concepts: Boxplots are often useful for depicting range and central tendency across many combinations of parameter values. Color, shape, line type to encode different factors Small multiples (faceting) to encode further factors (e.g., varying sample size) 8.5.1 Example 1 Bias of biserial correlation estimate from an extreme groups design \\(96 \\times 2 \\times 5 \\times 5\\) factorial design (true correlation, cut-off type, cut-off percentile, sample size) Source: Pustejovsky, J. E. (2014). Converting from d to r to z when the design uses extreme groups, dichotomization, or experimental control. Psychological Methods, 19(1), 92-112. 8.5.2 Example 2 Type-I error rates of small-sample corrected F-tests based on cluster-robust variance estimation in meta-regression Comparison of 5 different small-sample corrections Complex experimental design, varying sample size (\\(m\\)) dimension of hypothesis (\\(q\\)) covariates tested degree of model mis-specification Source: Tipton, E., &amp; Pustejovsky, J. E. (2015). Small-sample adjustments for tests of moderators and model fit using robust variance estimation in meta-regression. Journal of Educational and Behavioral Statistics, 40(6), 604-634. 8.5.3 Example 3 (Pustejovsky &amp; Swan, 2014) Coverage of parametric bootstrap confidence intervals for momentary time sampling data Compare maximum likelihood estimators to posterior mode (penalized likelihood) estimators of prevalence 2-dimensional parameter space: prevalence (19 levels) incidence (10 levels) Sample size (15 levels) Source: Pustejovsky, J. E., &amp; Swan, D. M. (2015). Four methods for analyzing partial interval recording data, with application to single-case research. Multivariate Behavioral Research, 50(3), 365-380. ## ------------------------------------------------------------------------- ## You have loaded plyr after dplyr - this is likely to cause problems. ## If you need functions from both plyr and dplyr, please load plyr first, then dplyr: ## library(plyr); library(dplyr) ## ------------------------------------------------------------------------- ## ## Attaching package: &#39;plyr&#39; ## The following objects are masked from &#39;package:dplyr&#39;: ## ## arrange, count, desc, failwith, id, mutate, rename, summarise, ## summarize 8.6 Modeling Simulations are designed experiments, often with a full factorial structure Helpful tools: ANOVA can be useful for understanding major sources of variation in simulation results (e.g., identifying which factors have negligible/minor influence on the bias of an estimator). Smoothing (e.g., local linear regression) over continuous factors 8.6.1 Example 1 Bias of biserial correlation estimate from an extreme groups design \\(96 \\times 2 \\times 5 \\times 5\\) factorial design (true correlation, cut-off type, cut-off percentile, sample size) anova_table &lt;- aov(bias ~ rho * p1 * fixed * n, data = r_F) summary(anova_table) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## rho 1 0.002444 0.002444 1673.25 &lt;2e-16 *** ## p1 4 0.023588 0.005897 4036.41 &lt;2e-16 *** ## fixed 1 0.015858 0.015858 10854.52 &lt;2e-16 *** ## n 4 0.013760 0.003440 2354.60 &lt;2e-16 *** ## rho:p1 4 0.001722 0.000431 294.71 &lt;2e-16 *** ## rho:fixed 1 0.003440 0.003440 2354.69 &lt;2e-16 *** ## p1:fixed 4 0.001683 0.000421 287.98 &lt;2e-16 *** ## rho:n 4 0.002000 0.000500 342.31 &lt;2e-16 *** ## p1:n 16 0.019810 0.001238 847.51 &lt;2e-16 *** ## fixed:n 4 0.013359 0.003340 2285.97 &lt;2e-16 *** ## rho:p1:fixed 4 0.000473 0.000118 80.87 &lt;2e-16 *** ## rho:p1:n 16 0.001470 0.000092 62.91 &lt;2e-16 *** ## rho:fixed:n 4 0.002929 0.000732 501.23 &lt;2e-16 *** ## p1:fixed:n 16 0.001429 0.000089 61.12 &lt;2e-16 *** ## rho:p1:fixed:n 16 0.000429 0.000027 18.36 &lt;2e-16 *** ## Residuals 4700 0.006866 0.000001 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 library(lsr) # etaSquared(anova_table) 8.7 Presentation Present selected results that clearly illustrate the main findings from the study and anything unusual/anomolous. In the text of your write-up, include examples that make specific numerical comparisons. Include supplementary materials containing additional figures and analysis complete simulation results reproducible code for running the simulation and doing the analysis "]
]
